{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.34.4",
  "scores": {
    "validation": [
      {
        "accuracy": 0.038965,
        "f1": 0.031759,
        "f1_weighted": 0.033955,
        "scores_per_experiment": [
          {
            "accuracy": 0.037109,
            "f1": 0.02681,
            "f1_weighted": 0.030167
          },
          {
            "accuracy": 0.035645,
            "f1": 0.033501,
            "f1_weighted": 0.031632
          },
          {
            "accuracy": 0.034668,
            "f1": 0.030814,
            "f1_weighted": 0.03409
          },
          {
            "accuracy": 0.03418,
            "f1": 0.035334,
            "f1_weighted": 0.031582
          },
          {
            "accuracy": 0.037598,
            "f1": 0.033724,
            "f1_weighted": 0.036673
          },
          {
            "accuracy": 0.039062,
            "f1": 0.032819,
            "f1_weighted": 0.034766
          },
          {
            "accuracy": 0.05127,
            "f1": 0.031275,
            "f1_weighted": 0.04308
          },
          {
            "accuracy": 0.043945,
            "f1": 0.035068,
            "f1_weighted": 0.036199
          },
          {
            "accuracy": 0.043457,
            "f1": 0.029931,
            "f1_weighted": 0.03313
          },
          {
            "accuracy": 0.032715,
            "f1": 0.028308,
            "f1_weighted": 0.028234
          }
        ],
        "main_score": 0.038965,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.036621,
        "f1": 0.027271,
        "f1_weighted": 0.028022,
        "scores_per_experiment": [
          {
            "accuracy": 0.029785,
            "f1": 0.016102,
            "f1_weighted": 0.01771
          },
          {
            "accuracy": 0.029785,
            "f1": 0.030138,
            "f1_weighted": 0.021196
          },
          {
            "accuracy": 0.030273,
            "f1": 0.028237,
            "f1_weighted": 0.027458
          },
          {
            "accuracy": 0.037598,
            "f1": 0.028327,
            "f1_weighted": 0.031769
          },
          {
            "accuracy": 0.035645,
            "f1": 0.032784,
            "f1_weighted": 0.034832
          },
          {
            "accuracy": 0.039062,
            "f1": 0.029545,
            "f1_weighted": 0.029664
          },
          {
            "accuracy": 0.05127,
            "f1": 0.028413,
            "f1_weighted": 0.038572
          },
          {
            "accuracy": 0.03125,
            "f1": 0.02043,
            "f1_weighted": 0.021198
          },
          {
            "accuracy": 0.04541,
            "f1": 0.035114,
            "f1_weighted": 0.0315
          },
          {
            "accuracy": 0.036133,
            "f1": 0.02362,
            "f1_weighted": 0.026316
          }
        ],
        "main_score": 0.036621,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 40.24768376350403,
  "kg_co2_emissions": null
}