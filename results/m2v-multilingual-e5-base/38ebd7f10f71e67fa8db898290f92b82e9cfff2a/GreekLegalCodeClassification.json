{
  "dataset_revision": "de0fdb34424f07d1ac6f0ede23ee0ed44bd9f5d1",
  "task_name": "GreekLegalCodeClassification",
  "mteb_version": "1.34.4",
  "scores": {
    "validation": [
      {
        "accuracy": 0.114062,
        "f1": 0.078966,
        "f1_weighted": 0.114172,
        "scores_per_experiment": [
          {
            "accuracy": 0.110352,
            "f1": 0.073221,
            "f1_weighted": 0.117748
          },
          {
            "accuracy": 0.120605,
            "f1": 0.084042,
            "f1_weighted": 0.120885
          },
          {
            "accuracy": 0.112793,
            "f1": 0.078292,
            "f1_weighted": 0.111637
          },
          {
            "accuracy": 0.102051,
            "f1": 0.076531,
            "f1_weighted": 0.100868
          },
          {
            "accuracy": 0.10791,
            "f1": 0.074261,
            "f1_weighted": 0.112032
          },
          {
            "accuracy": 0.122559,
            "f1": 0.078377,
            "f1_weighted": 0.1238
          },
          {
            "accuracy": 0.12793,
            "f1": 0.08847,
            "f1_weighted": 0.12811
          },
          {
            "accuracy": 0.112793,
            "f1": 0.077798,
            "f1_weighted": 0.106347
          },
          {
            "accuracy": 0.111816,
            "f1": 0.08097,
            "f1_weighted": 0.10984
          },
          {
            "accuracy": 0.111816,
            "f1": 0.077698,
            "f1_weighted": 0.110456
          }
        ],
        "main_score": 0.114062,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.10918,
        "f1": 0.078966,
        "f1_weighted": 0.109889,
        "scores_per_experiment": [
          {
            "accuracy": 0.120605,
            "f1": 0.08247,
            "f1_weighted": 0.122988
          },
          {
            "accuracy": 0.116699,
            "f1": 0.086256,
            "f1_weighted": 0.116517
          },
          {
            "accuracy": 0.101074,
            "f1": 0.068128,
            "f1_weighted": 0.100642
          },
          {
            "accuracy": 0.115234,
            "f1": 0.085657,
            "f1_weighted": 0.117065
          },
          {
            "accuracy": 0.102051,
            "f1": 0.073687,
            "f1_weighted": 0.106748
          },
          {
            "accuracy": 0.105469,
            "f1": 0.07627,
            "f1_weighted": 0.108454
          },
          {
            "accuracy": 0.114746,
            "f1": 0.083478,
            "f1_weighted": 0.112597
          },
          {
            "accuracy": 0.104492,
            "f1": 0.0749,
            "f1_weighted": 0.099343
          },
          {
            "accuracy": 0.098145,
            "f1": 0.073633,
            "f1_weighted": 0.097972
          },
          {
            "accuracy": 0.113281,
            "f1": 0.085183,
            "f1_weighted": 0.116564
          }
        ],
        "main_score": 0.10918,
        "hf_subset": "default",
        "languages": [
          "ell-Grek"
        ]
      }
    ]
  },
  "evaluation_time": 74.56546998023987,
  "kg_co2_emissions": null
}