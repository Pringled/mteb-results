{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.34.4",
  "scores": {
    "validation": [
      {
        "accuracy": 0.27749,
        "f1": 0.272523,
        "f1_weighted": 0.272511,
        "scores_per_experiment": [
          {
            "accuracy": 0.280273,
            "f1": 0.279756,
            "f1_weighted": 0.279736
          },
          {
            "accuracy": 0.287109,
            "f1": 0.286354,
            "f1_weighted": 0.286345
          },
          {
            "accuracy": 0.237305,
            "f1": 0.220373,
            "f1_weighted": 0.220324
          },
          {
            "accuracy": 0.31543,
            "f1": 0.311936,
            "f1_weighted": 0.311906
          },
          {
            "accuracy": 0.299316,
            "f1": 0.289952,
            "f1_weighted": 0.289902
          },
          {
            "accuracy": 0.269043,
            "f1": 0.261886,
            "f1_weighted": 0.261878
          },
          {
            "accuracy": 0.242188,
            "f1": 0.236291,
            "f1_weighted": 0.23633
          },
          {
            "accuracy": 0.300781,
            "f1": 0.298809,
            "f1_weighted": 0.298824
          },
          {
            "accuracy": 0.28418,
            "f1": 0.284015,
            "f1_weighted": 0.283979
          },
          {
            "accuracy": 0.259277,
            "f1": 0.255861,
            "f1_weighted": 0.255882
          }
        ],
        "main_score": 0.27749,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.281934,
        "f1": 0.276991,
        "f1_weighted": 0.276987,
        "scores_per_experiment": [
          {
            "accuracy": 0.263672,
            "f1": 0.265748,
            "f1_weighted": 0.265756
          },
          {
            "accuracy": 0.274902,
            "f1": 0.273654,
            "f1_weighted": 0.273654
          },
          {
            "accuracy": 0.256348,
            "f1": 0.237023,
            "f1_weighted": 0.236959
          },
          {
            "accuracy": 0.304199,
            "f1": 0.30481,
            "f1_weighted": 0.304797
          },
          {
            "accuracy": 0.318848,
            "f1": 0.310821,
            "f1_weighted": 0.310781
          },
          {
            "accuracy": 0.269531,
            "f1": 0.25989,
            "f1_weighted": 0.259863
          },
          {
            "accuracy": 0.251465,
            "f1": 0.24328,
            "f1_weighted": 0.243334
          },
          {
            "accuracy": 0.320801,
            "f1": 0.318664,
            "f1_weighted": 0.318704
          },
          {
            "accuracy": 0.297363,
            "f1": 0.297775,
            "f1_weighted": 0.297753
          },
          {
            "accuracy": 0.262207,
            "f1": 0.258241,
            "f1_weighted": 0.258271
          }
        ],
        "main_score": 0.281934,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 0.8992786407470703,
  "kg_co2_emissions": null
}